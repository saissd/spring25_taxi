{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14614ab5-3fa5-4b6a-ad2b-9758d1f363d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd9e12e-0bb9-4d5e-81cc-f126630d8952",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Error creating dataset. Could not read schema from '../data/processed/rides_2023_01.parquet'. Is this a 'parquet' file?: Could not open Parquet input source '../data/processed/rides_2023_01.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m year = \u001b[32m2023\u001b[39m\n\u001b[32m      7\u001b[39m path = Path(\u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m) / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrides_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m table = \u001b[43mpq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m rides = table.to_pandas()\n\u001b[32m     11\u001b[39m rides.iloc[\u001b[32m1000\u001b[39m:\u001b[32m1020\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/parquet/core.py:1793\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1787\u001b[39m     warnings.warn(\n\u001b[32m   1788\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_legacy_dataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1790\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/parquet/core.py:1371\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1368\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1369\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/dataset.py:794\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    783\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    784\u001b[39m     schema=schema,\n\u001b[32m    785\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    791\u001b[39m )\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/dataset.py:486\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    478\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    479\u001b[39m     partitioning=partitioning,\n\u001b[32m    480\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    481\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    482\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    483\u001b[39m )\n\u001b[32m    484\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/_dataset.pyx:3138\u001b[39m, in \u001b[36mpyarrow._dataset.DatasetFactory.finish\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv3/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: Error creating dataset. Could not read schema from '../data/processed/rides_2023_01.parquet'. Is this a 'parquet' file?: Could not open Parquet input source '../data/processed/rides_2023_01.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "month = 1\n",
    "year = 2023\n",
    "path = Path(\"..\") / \"data\" / \"processed\" / f\"rides_{year}_{month:02}.parquet\"\n",
    "\n",
    "table = pq.read_table(path)\n",
    "rides = table.to_pandas()\n",
    "rides.iloc[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb842b7c-e891-4413-b353-738365814ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e318239-22ad-4edd-9dee-f9c41ab6cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides[\"pickup_hour\"] = rides[\"pickup_datetime\"].dt.floor('h')\n",
    "rides.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe31ae2-62be-4c23-a98d-e6e34d8002eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_rides = rides.groupby([\"pickup_hour\", \"pickup_location_id\"]).size().reset_index()\n",
    "agg_rides.rename(columns={0: \"rides\"}, inplace=True)\n",
    "agg_rides.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942d008-2f0c-4420-bdec-ce7afcbdd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset\n",
    "data = {\n",
    "    \"hour\": [\"2025-01-28 08:00\", \"2025-01-28 08:00\", \"2025-01-28 09:00\", \"2025-01-28 10:00\"],\n",
    "    \"location_id\": [1, 2, 1, 2],\n",
    "    \"rides\": [10, 5, 8, 12]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df[\"hour\"] = pd.to_datetime(df[\"hour\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583cba5e-e814-49e3-a3ed-a5cf252008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the hours\n",
    "hour_col = \"hour\"\n",
    "full_hours = pd.date_range(\n",
    "    start=df[hour_col].min(),\n",
    "    end=df[hour_col].max(),\n",
    "    freq=\"h\"\n",
    ")\n",
    "\n",
    "full_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a8f0b-2339-469a-9d57-6fb6965ce91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique location ids\n",
    "location_col = \"location_id\"\n",
    "all_locations = df[location_col].unique()\n",
    "all_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8695d0-7696-4830-a538-a1019c7dfda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the complete data\n",
    "complete_data = []\n",
    "rides_col = \"rides\"\n",
    "# Iterate over all combinations of hours and locations\n",
    "for hour in full_hours:\n",
    "    for location in all_locations:\n",
    "        # Check if the combination exists in the original DataFrame\n",
    "        subset = df[(df[hour_col] == hour) & (df[location_col] == location)]\n",
    "        if not subset.empty:\n",
    "            # If the combination exists, append the row\n",
    "            complete_data.append(subset.iloc[0].to_dict())\n",
    "        else:\n",
    "            # If the combination is missing, append a row with 0 rides\n",
    "            complete_data.append({hour_col: hour, location_col: location, rides_col: 0})\n",
    "\n",
    "df_complete = pd.DataFrame(complete_data)\n",
    "df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d9c8b-b4ca-4384-9137-e1f6747e9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.sort_values([\"location_id\", \"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84806b2d-f2f5-4a86-8f6e-9194bcd65e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full combinations of hours and locations\n",
    "full_combinations = pd.DataFrame(\n",
    "    [(hour, location) for hour in full_hours for location in all_locations],\n",
    "    columns=[\"hour\", \"location_id\"]\n",
    ")\n",
    "\n",
    "full_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce7cba-0acb-47b9-bdb9-a96f1baba2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c9e8e-449e-403b-aa33-8dab82d29db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the original dataset\n",
    "merged_df = pd.merge(full_combinations, df, on=[\"hour\", \"location_id\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc545b-b026-4428-8721-4ef6e92616f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"rides\"] = merged_df[\"rides\"].fillna(0).astype(int)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66793ac4-08a1-4cda-b559-92158c2d9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_rides_full_range(df, hour_col, location_col, rides_col):\n",
    "    \"\"\"\n",
    "    Fills in missing rides for all hours in the range and all unique locations.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with columns [hour_col, location_col, rides_col]\n",
    "    - hour_col: Name of the column containing hourly timestamps\n",
    "    - location_col: Name of the column containing location IDs\n",
    "    - rides_col: Name of the column containing ride counts\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with missing hours and locations filled in with 0 rides\n",
    "    \"\"\"\n",
    "    # Ensure the hour column is in datetime format\n",
    "    df[hour_col] = pd.to_datetime(df[hour_col])\n",
    "\n",
    "    # Get the full range of hours (from min to max) with hourly frequency\n",
    "    full_hours = pd.date_range(\n",
    "        start=df[hour_col].min(),\n",
    "        end=df[hour_col].max(),\n",
    "        freq=\"h\"\n",
    "    )\n",
    "\n",
    "    # Get all unique location IDs\n",
    "    all_locations = df[location_col].unique()\n",
    "\n",
    "    # Create a DataFrame with all combinations of hours and locations\n",
    "    full_combinations = pd.DataFrame(\n",
    "        [(hour, location) for hour in full_hours for location in all_locations],\n",
    "        columns=[hour_col, location_col]\n",
    "    )\n",
    "\n",
    "    # Merge the original DataFrame with the full combinations DataFrame\n",
    "    merged_df = pd.merge(full_combinations, df, on=[hour_col, location_col], how='left')\n",
    "\n",
    "    # Fill missing rides with 0\n",
    "    merged_df[rides_col] = merged_df[rides_col].fillna(0).astype(int)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc424e-de87-4549-894b-8b8df5ab0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5766757-b907-476f-bb5f-dd32add70096",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_col = \"pickup_hour\"\n",
    "location_col = \"pickup_location_id\"\n",
    "rides_col = \"rides\"\n",
    "agg_data_filled = fill_missing_rides_full_range(agg_rides, hour_col, location_col, rides_col).sort_values([\"pickup_location_id\", \"pickup_hour\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75283a-c7c4-42aa-81f3-20c4b0f9bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 2000)\n",
    "agg_data_filled.head(1489)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406aece-7f98-4261-8356-ae408aeef760",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6d585-5668-45c8-a3bb-2d6c6c791deb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "):\n",
    "\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"rides\",\n",
    "        color=\"pickup_location_id\",\n",
    "        template=\"none\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6480c8-f7fc-42f4-a8ff-7bd915335830",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rides(agg_data_filled, locations=[42, 43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc896514-f140-4092-89fd-18b609e8a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 1\n",
    "year = 2023\n",
    "path = Path(\"..\") / \"data\" / \"processed\" / f\"ts_data_{year}_{month:02}.parquet\"\n",
    "\n",
    "agg_data_filled.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c0bed-3c0e-43e8-a9d6-c9755f247bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
